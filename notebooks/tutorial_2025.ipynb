{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# From Messy Data to Medical Insights: Creating KGs for Drug Repurposing\n","### Authors: Matilde Pato, Ana Carolina Pereira and Nuno Datia\n","### [womenENcourage2025](https://womencourage.acm.org/2025/), 18th September\n","---\n","\n","## Agenda:\n","\n","<ul>\n","  <li><a href=\"#1\">1. Data retrieval and cleaning</a>\n","    <ul>\n","      <li><a href=\"#1.1\">1.1. Import libraries</a></li>\n","      <li><a href=\"#1.2\">1.2. Retrieve DailyMed, Purple Book and Orange Book</a></li>\n","      <li><a href=\"#1.3\">1.3. Exploring the datasets</a></li>\n","      <li><a href=\"#1.4\">1.4. Convert all files to JSON using jsonifyer package</a></li>\n","      <li><a href=\"#1.5\">1.5. Text preprocessing</a></li>\n","    </ul>\n","  <li><a href=\"#2\">2. Named Entity Recognition (NER) + Named Entity Linking (NEL)</a></li>\n","    <ul>\n","      <li><a href=\"#2.1\">2.1. Import libraries</a></li>\n","      <li><a href=\"#2.2\">2.2. Configure MER</a></li>\n","      <li><a href=\"#2.3\">2.3. Extract the entities in a sample</a></li>\n","      <li><a href=\"#2.5\">2.4. Other ontology</a></li>\n","    </ul>\n","</ul>\n","<ul>\n","   <li><a href=\"#3\">3. Knowledge Graph</a></li>\n","   <ul>\n","      <li><a href=\"#3.1\">3.1. Import Libraries</a></li>\n","      <li><a href=\"#3.2\">3.2. Enter your Neo4j credentials </a></li>\n","      <li><a href=\"#3.3\">3.3. Neo4j Handler class</a></li>\n","      <li><a href=\"#3.4\">3.4. Utility functions for data processing </a></li>\n","      <li><a href=\"#3.5\">3.5. Establish connection to Neo4j </a></li>\n","      <li><a href=\"#3.6\">3.6. Import data from JSON files </a></li>\n","      <li><a href=\"#3.7\">3.7. Close Neo4j connection </a></li>\n","    </ul>\n","</ul>\n"],"metadata":{"id":"GG64IhgciUqU"}},{"cell_type":"markdown","source":["<a id=\"1\"></a>\n","\n","## 1. Data Retrieval and Text Pre-Processing\n","\n","**Goal #1**: To retrieve the\n","1. [DailyMed](https://dailymed.nlm.nih.gov/dailymed/) database contains labeling submitted to the Food and Drug Administration (FDA) by companies for prescription drugs and biological products for human use. We will select a sample of complete drug labels (indications, contraindications, warningsandprecautions, ingredients) to build a knowledge graph.\n","2. [Purple Book](https://purplebooksearch.fda.gov/) database contains information about all FDA-licensed biological products regulated by the CDER, and FDA-licensed allergenic, cellular and gene therapy, hematologic, and vaccine products regulated by CBER.\n","3. [Orange Book](https://www.fda.gov/drugs/drug-approvals-and-databases/approved-drug-products-therapeutic-equivalence-evaluations-orange-book) (Approved Drug Products With Therapeutic Equivalence Evaluations) is composed of approved prescription drug products with therapeutic equivalence evaluations (Prescription Drug Product List) and others.\n","\n","To convert different types of files (XML, CSV, TXT) into JSON format, we are going to apply the [JSONIFYER](https://pypi.org/project/jsonifyer/) tool.\n","\n","**Goal #2**: **Text preprocessing** constitutes a fundamental stage in [Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP), wherein raw textual data is systematically cleaned and transformed into a structured format suitable for computational analysis and machine learning applications. This process improves data quality and analytical utility by eliminating noise, standardizing linguistic forms, and reducing textual complexity."],"metadata":{"id":"gGWfaDfLjGvg"}},{"cell_type":"markdown","source":["<a id=\"1.1\"></a>\n","### 1.1 Import libraries"],"metadata":{"id":"BpFl0KCNpnTL"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import re\n","import json\n","import string\n","import time\n","from pathlib import Path\n","\n","import xml.etree.ElementTree as ET"],"metadata":{"id":"j6pyZPQDQlFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install the jsonifyer package\n","!pip install jsonifyer"],"metadata":{"id":"BxALmShlTqHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import specific conversion functions from jsonifyer package\n","# These functions will help us convert different file formats into JSON:\n","from jsonifyer import (\n","    convert_txt,    # For plain text file conversion\n","    convert_csv,    # For comma-separated values file conversion\n","    convert_xml     # For XML markup file conversion\n",")"],"metadata":{"id":"h7YjXbmtToSx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"1.2\"></a>\n","### 1.2. Retrieve DailyMed, Purple Book and Orange Book\n","\n","DailyMed is a large dataset that contains **153,971** labels submitted to the FDA, so in this tutorial, we are going to use a **smaller version of the dataset**.\n","\n","This version is located under the directory \"drugs_small\"."],"metadata":{"id":"7FxjM1tIqKOX"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"R8YTOzdGjr0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# change working directory\n","os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Tutorial WomenEncourage2025/\")\n","# Define the base directory for our project\n","BASE_DIR = Path(os.getcwd())\n","\n","# Set up input and output folder paths\n","input_folder = BASE_DIR / 'drugs_small'\n","output_folder = BASE_DIR / 'json'\n","\n","# Create output directories for each file type\n","# This mirrors the input structure: xml_files, csv_files, txt_files\n","file_types = ['xml_files', 'csv_files', 'txt_files']\n","\n","print(\"Creating output directory structure...\")\n","for dir_type in file_types:\n","    output_dir = output_folder / dir_type\n","    os.makedirs(output_dir, exist_ok=True)\n","    print(f\"‚úì Created directory: {output_dir}\")\n","\n","# Create tracking files to keep record of processed files\n","# These files prevent duplicate processing of the same data\n","repeated_files = {\n","    'xml_files': BASE_DIR / 'xml_processed.txt',\n","    'csv_files': BASE_DIR / 'csv_processed.txt',\n","    'txt_files': BASE_DIR / 'txt_processed.txt'\n","}\n","\n","print(\"Creating tracking files...\")\n","for file_type, file_path in repeated_files.items():\n","    if not file_path.exists():\n","        file_path.touch()  # Create empty file\n","        print(f\"‚úì Created tracking file: {file_path}\")\n","    else:\n","        print(f\"‚Ñπ Tracking file already exists: {file_path}\")"],"metadata":{"id":"04HrXe6fVZT8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"1.3\"></a>\n","\n","### 1.3. Exploring the datasets"],"metadata":{"id":"-8bDmBXvLpAY"}},{"cell_type":"code","source":["# DailyMed\n","# Read and parse XML\n","xml_file_path = Path(input_folder) / \"xml_files/0d1f10eb-de2e-49c3-bbea-2cafbafe05b1.xml\"\n","tree = ET.parse(xml_file_path)\n","root = tree.getroot()\n","\n","print(f\"üìã ROOT ELEMENT: <{root.tag}>\")\n","print(f\"Root attributes: {root.attrib}\")\n","print(\"-\" * 50)\n","\n","element_attrs = {}\n","for elem in root.iter():\n","    tag = elem.tag\n","    if tag not in element_attrs:\n","        element_attrs[tag] = set()\n","    element_attrs[tag].update(elem.attrib.keys())\n","for tag, attrs in element_attrs.items():\n","    if attrs:\n","        print(f\"{tag}: {', '.join(attrs)}\")\n","    else:\n","        print(f\"{tag}: (no attributes)\")"],"metadata":{"id":"6CkHSaN4I0g_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print first few lines of XML for manual inspection\n","with open(xml_file_path, 'r', encoding='utf-8') as f:\n","    lines = f.readlines()\n","\n","print(\"\\nüìñ FIRST 20 LINES OF XML:\")\n","print(\"-\" * 30)\n","for i, line in enumerate(lines[:20], 1):\n","    print(f\"{i:2d}: {line.rstrip()}\")"],"metadata":{"id":"ou0GEDD6J-qF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read the file from Purple book database\n","csv_files = Path(input_folder) / \"csv_files/purplebook-search-may-data-download.csv\"\n","purple_book = pd.read_csv(csv_files, sep = ',', quotechar = '\"',\n","                          encoding = 'utf-8', dtype=str)\n","purple_book.head(4)"],"metadata":{"id":"O7sQzXGztGBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read the products.txt file (Orange Book data)\n","txt_file_path = Path(input_folder) / \"txt_files/products.txt\"\n","\n","# Method 1: Manual inspection of raw file content\n","with open(txt_file_path, 'r', encoding='utf-8') as f:\n","    lines = f.readlines()\n","for i, line in enumerate(lines[:5], 1):\n","    print(f\"{i:2d}: {line.rstrip()}\")\n","# Method 2: Using pandas to read the delimited file\n","orange_book = pd.read_csv(txt_file_path, sep='~', dtype=str, encoding='utf-8')\n","print(orange_book.head(4))"],"metadata":{"id":"axYbaaHZPY6u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"1.4\"></a>\n","\n","### 1.4. Convert all files to JSON using **jsonifyer** package"],"metadata":{"id":"5v7lMxxSqVcM"}},{"cell_type":"code","source":["# Configure XML processing parameters\n","print(\"Configuring XML processing parameters...\")\n","# Define XML namespaces - these help parse XML documents correctly\n","namespaces = {'': 'urn:hl7-org:v3'}\n","\n","# Map fields to their XPath locations in the XML\n","# XPath is a way to navigate XML document structure\n","field_map = {\n","    'id': './/id/@root',\n","    'code.code': './/code/@code',\n","    'code.codeSystem': './/code/@codeSystem',\n","    'code.displayName': './/code/@displayName',\n","    'organization': './/author/assignedEntity/representedOrganization/name',\n","    'name': './/component/structuredBody/component/section/subject/manufacturedProduct/manufacturedProduct/name',\n","    'effectiveTime': './/effectiveTime/@value',\n","    'ingredients.name': './/component/structuredBody/component/section/subject/manufacturedProduct/manufacturedProduct/ingredient/ingredientSubstance/name',\n","    'ingredients.code': './/component/structuredBody/component/section/subject/manufacturedProduct/manufacturedProduct/ingredient/ingredientSubstance/code/@code',\n","}\n","\n","# Define section codes for specific medical document sections\n","section_codes = {\n","    'indications': '34067-9',\n","    'contraindications': '34068-7',\n","    'warningsAndPrecautions': '34069-5',\n","    'adverseReactions': '34070-3'\n","}\n","\n","# Define paired fields that should be processed together\n","# This is useful for ingredients that have both names and codes\n","pairs = {\n","    'ingredients.name': [\n","        './/component/structuredBody/component/section/subject/manufacturedProduct/manufacturedProduct/ingredient/ingredientSubstance',\n","        'name'\n","    ],\n","    'ingredients.code': [\n","        './/component/structuredBody/component/section/subject/manufacturedProduct/manufacturedProduct/ingredient/ingredientSubstance',\n","        'code/@code'\n","    ],\n","}\n","\n","xml_input_dir = input_folder / 'xml_files'\n","xml_output_dir = output_folder / 'xml_files'\n","\n","print(\"\\n\" + \"-\" * 40)\n","print(\"PROCESSING XML FILES\")\n","print(\"-\" * 40)\n","# Call the convert_xml function with all our configured parameters\n","result = convert_xml(\n","    directory_path=str(xml_input_dir),\n","    repeated_path=str(repeated_files['xml_files']),\n","    repeated_item='name',  # Field to check for duplicates\n","    output_path=str(xml_output_dir),\n","    converter=\"python\",  # Use Python XML parser\n","    field_map=field_map,\n","    extra_fields=section_codes,\n","    namespaces=namespaces,\n","    pairs=pairs,\n","    root=\"document\"  # Root element of XML documents\n",")\n","\n","print(\"\\n\" + \"-\" * 40)\n","print(\"COMPLETED XML FILES\")\n","print(\"-\" * 40)\n","# Count actual files to show real results\n","xml_files = [f for f in os.listdir(xml_input_dir) if f.endswith('.xml')]\n","json_files = [f for f in os.listdir(xml_output_dir) if f.endswith('.json')]\n","\n","total_xml = len(xml_files)\n","total_json = len(json_files)\n","\n","print(f\"‚úì Successfully converted {total_json} out of {total_xml} XML files\")\n","print(f\"‚úì Success rate: {(total_json/total_xml)*100:.1f}%\")\n","\n","if total_json < total_xml:\n","    print(f\"{total_xml - total_json} files had conversion issues\")"],"metadata":{"id":"0oedpRH2q1gH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process CSV FILES\n","csv_input_dir = input_folder / 'csv_files'\n","csv_output_dir = output_folder / 'csv_files'\n","\n","print(\"\\n\" + \"-\" * 40)\n","print(\"PROCESSING CSV FILES\")\n","print(\"-\" * 40)\n","\n","# Process each CSV file in the directory\n","for filename in os.listdir(csv_input_dir):\n","# Skip files that aren't CSV format\n","  if not filename.lower().endswith('.csv'):\n","      print(f\"‚Ñπ Skipping non-CSV file: {filename}\")\n","      continue\n","\n","  filepath = csv_input_dir / filename\n","  print(f\"INFO: Processing CSV file: {filename}...\")\n","\n","  # Convert individual CSV file to JSON\n","  result = convert_csv(\n","      file_path=str(filepath),\n","      output_path=str(csv_output_dir),\n","      repeated_path=str(repeated_files['csv_files']),\n","      repeated_item='Proper Name',  # Column to check for duplicates\n","      skiprows=3  # Skip first 3 rows (likely headers/metadata)\n","  )\n","\n","  print(f\"INFO: Conversion result for {filename}: {result}\")\n","\n","  # Display success message\n","  if isinstance(result, dict) and 'message' in result:\n","      print(f\"{result['message']}\")\n","  else:\n","      print(f\"{filename} processed successfully\")"],"metadata":{"id":"Iek0WlGIuyWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process TXT files\n","txt_input_dir = input_folder / 'txt_files'\n","txt_output_dir = output_folder / 'txt_files'\n","\n","print(\"\\n\" + \"-\" * 40)\n","print(\"PROCESSING TXT FILES\")\n","print(\"-\" * 40)\n","\n","# Process each text file in the directory\n","for filename in os.listdir(txt_input_dir):\n","  # Skip files that aren't text format\n","  if not filename.lower().endswith('.txt'):\n","      print(f\"Skipping non-TXT file: {filename}\")\n","      continue\n","\n","  filepath = txt_input_dir / filename\n","  print(f\"INFO: Processing TXT file: {filename}...\")\n","\n","  # Convert delimited text file to JSON\n","  result = convert_txt(\n","      file_path=str(filepath),\n","      output_path=str(txt_output_dir),\n","      repeated_path=str(repeated_files['txt_files']),\n","      repeated_item='Ingredient',  # Field to check for duplicates\n","      delimiter='~'  # Custom delimiter (tilde instead of comma)\n","  )\n","\n","  print(f\"INFO: Conversion result for {filename}: {result}\")\n","\n","  # Display success message\n","  if isinstance(result, dict) and 'message' in result:\n","      print(f\"{result['message']}\")\n","  else:\n","      print(f\"{filename} processed successfully\")"],"metadata":{"id":"kRnJoy7SjeIk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"1.5\"></a>\n","### 1.5. Text Preprocessing\n","\n","Raw pharmaceutical data from **DailyMed**, **Purple Book**, and **Orange Book** contains inconsistent formatting, medical abbreviations, and varying drug name spellings that hinder knowledge graph construction. Our preprocessing pipeline standardizes this text to enable accurate drug-disease relationship extraction.\n","\n","#### Key Processing Steps\n","\n","1. Encoding fixes: Resolve character encoding issues and HTML entities\n","2. Medical standardization: Expand abbreviations (mg ‚Üí milligram) and normalize drug names (paracetamol ‚Üí acetaminophen)\n","3. Text cleaning: Remove stopwords, normalize punctuation, and standardize case\n","4. Field processing: Clean specific pharmaceutical fields (ingredients, indications, contraindications, warnings)"],"metadata":{"id":"MewOr3HK8leW"}},{"cell_type":"code","source":["# Install required packages (run this cell first in Colab)\n","!pip install nltk"],"metadata":{"id":"xUxiWQ9-80qo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","\n","# Download required NLTK resources\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","nltk.download('punkt_tab')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","all_stopwords = stopwords.words('english')"],"metadata":{"id":"2l8Qmpr88jHl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Processes one pharmaceutical JSON file"],"metadata":{"id":"ZOIkIDNBwW12"}},{"cell_type":"code","source":["# Define preprocessing dictionaries\n","BIO_ABBREVIATIONS = {\n","    'mg': 'milligram',\n","    'ml': 'milliliter',\n","    'iv': 'intravenous',\n","    'po': 'oral',\n","    'bid': 'twice daily',\n","    'tid': 'three times daily',\n","    'qid': 'four times daily',\n","    'prn': 'as needed',\n","    'stat': 'immediately'\n","}\n","\n","DRUG_CORRECTIONS = {\n","    'acetaminophen': 'acetaminophen',\n","    'paracetamol': 'acetaminophen',\n","    'tylenol': 'acetaminophen',\n","    'ibuprofen': 'ibuprofen',\n","    'advil': 'ibuprofen',\n","    'motrin': 'ibuprofen'\n","}\n","\n","CUSTOM_STOPWORDS = [\n","    'patient', 'patients', 'medication', 'drug', 'treatment', 'therapy',\n","    'dose', 'dosage', 'administration', 'clinical', 'study', 'trial'\n","]\n","\n","# Fields to process\n","FIELDS_TO_PROCESS = [\n","    \"ingredients\",\n","    \"indications\",\n","    \"contraindications\",\n","    \"warningsAndPrecautions\",\n","    \"adverseReactions\"\n","]"],"metadata":{"id":"j-osdfW08Ag6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example of a file content\n","json_file_path = Path(output_folder) / \"xml_files/0d1f10eb-de2e-49c3-bbea-2cafbafe05b1.json\"\n","\n","#  Load JSON data\n","with open(json_file_path) as f:\n","    drugs_data = json.load(f)\n","\n","# Process each specified field\n","for field in FIELDS_TO_PROCESS:\n","    if field in drugs_data and isinstance(drugs_data[field], str):\n","        print(f\"\\nProcessing field: {field}\")\n","\n","        # Get the original text\n","        text = drugs_data[field]\n","        print(f\"Original text preview: {text[:50]}...\")\n","\n","        # Preserve original text\n","        drugs_data[f\"{field}_original\"] = text\n","\n","        # Step 1: Fix common encoding issues\n","        encoding_fixes = {\n","            '\\x92': \"'\", '\\x93': '\"', '\\x94': '\"',  # Smart quotes\n","            '\\x96': '-', '\\x97': '-', '\\xa0': ' ',  # Dashes and spaces\n","            '&amp;': '&', '&lt;': '<', '&gt;': '>'  # HTML entities\n","        }\n","\n","        for old, new in encoding_fixes.items():\n","            text = text.replace(old, new)\n","\n","        # Step 2: Convert to lowercase for processing\n","        text = text.lower()\n","\n","        # Step 3: Standardize punctuation\n","        # Normalize multiple dashes to single dash\n","        text = re.sub(r'-+', '-', text)\n","\n","        # Add spaces around punctuation for better tokenization\n","        for punct in [',', '.', ';', ':', '!', '?', '(', ')']:\n","            text = re.sub(f'\\\\{punct}', f' {punct} ', text)\n","\n","        # Normalize whitespace\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","\n","        # Step 4: Expand biomedical abbreviations\n","        for abbr, full_form in BIO_ABBREVIATIONS.items():\n","            # Use word boundaries to avoid partial matches\n","            pattern = rf'\\b{abbr.lower()}\\b'\n","            text = re.sub(pattern, full_form.lower(), text)\n","\n","        # Step 5: Correct drug name spellings\n","        words = word_tokenize(text)\n","        corrected_words = []\n","\n","        for word in words:\n","            # Check if word needs spelling correction\n","            if word in DRUG_CORRECTIONS:\n","                corrected_words.append(DRUG_CORRECTIONS[word])\n","                print(f\"Corrected: {word} ‚Üí {DRUG_CORRECTIONS[word]}\")\n","            else:\n","                corrected_words.append(word)\n","\n","        text = ' '.join(corrected_words)\n","\n","        # Step 6: Remove stopwords\n","        # Get standard English stopwords\n","        try:\n","            english_stops = set(stopwords.words('english'))\n","        except:\n","            # Fallback if NLTK stopwords not available\n","            english_stops = {'the', 'a', 'an', 'and', 'or', 'but', 'in',\n","                             'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n","\n","        # Combine with custom medical stopwords\n","        all_stopwords = english_stops.union(set(CUSTOM_STOPWORDS))\n","\n","        # Tokenize and filter stopwords\n","        tokens = word_tokenize(text)\n","        filtered_tokens = [token for token in tokens if token not in\n","                           all_stopwords and token not in string.punctuation]\n","\n","        cleaned_text = ' '.join(filtered_tokens)\n","        print(f\"Cleaned text preview: {cleaned_text[:100]}...\")\n","\n","        # Update the field with cleaned text\n","        drugs_data[field] = cleaned_text\n","\n","        print(f\"‚úì Successfully processed: {drugs_data}\")"],"metadata":{"id":"mJl2mU6gwVoY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Extra**: Pipeline for clean ALL data\n","\n","1. ```clean_biomedical_text(text)```\n","\n","    Purpose: Cleans raw pharmaceutical text\n","*   Fixes encoding issues (smart quotes, HTML entities)\n","*   Expands abbreviations (mg ‚Üí milligram, iv ‚Üí intravenous)\n","*   Standardizes drug names (paracetamol ‚Üí acetaminophen)\n","*   Removes stopwords and normalizes text\n","\n","\n","2. ```process_single_json_file(input_file, output_file)```\n","\n","    Purpose: Processes one pharmaceutical JSON file\n","*   Loads drug data from DailyMed/Purple Book format\n","*   Cleans specific fields: ingredients, indications, contraindications, warnings\n","*   Preserves original text in *_original fields\n","*   Saves cleaned version\n","\n","\n","3. ```process_file_wrapper(file_task)```\n","\n","    Purpose: Enables parallel processing\n","*   Wraps single file processing for multiprocessing\n","*   Handles errors without crashing entire batch\n","*   Tracks processing time and results\n","*   Returns success/failure status\n","\n","\n","4. ```collect_all_json_files(input_dir, output_dir)```\n","\n","    Purpose: File discovery and organization\n","*   Finds all JSON files in directory tree\n","*   Creates matching output directory structure\n","*   Returns list of file processing tasks\n","*   Preserves folder organization\n","\n","***Parallel Processing Flow***\n","```# Main batch processing function uses all four:\n","def preprocess_json_task_batch(input_dir, output_dir, batch_size=50):\n","    # 1. Find all files\n","    file_tasks = collect_all_json_files(input_dir, output_dir)\n","    \n","    # 2. Process in parallel batches\n","    with ProcessPoolExecutor() as executor:\n","        # 3. Each worker uses process_file_wrapper\n","        futures = [executor.submit(process_file_wrapper, task)\n","                  for task in batch_tasks]\n","        \n","        # 4. Which calls process_single_json_file\n","        # 5. Which calls clean_biomedical_text for each field```"],"metadata":{"id":"d_i7txZqyjLv"}},{"cell_type":"code","source":["def clean_biomedical_text(text):\n","    \"\"\"Simplified biomedical text cleaning\"\"\"\n","    # Basic encoding fixes\n","    encoding_fixes = {\n","        '\\x92': \"'\", '\\x93': '\"', '\\x94': '\"', '\\x96': '-',\n","        '\\x97': '-', '\\xa0': ' ', '&amp;': '&', '&lt;': '<', '&gt;': '>'\n","    }\n","    for old, new in encoding_fixes.items():\n","        text = text.replace(old, new)\n","\n","    # Lowercase and normalize punctuation\n","    text = text.lower()\n","    text = re.sub(r'-+', '-', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    # Expand abbreviations\n","    for abbr, full_form in BIO_ABBREVIATIONS.items():\n","        text = re.sub(rf'\\b{abbr}\\b', full_form, text)\n","\n","    # Tokenize and process\n","    try:\n","        tokens = word_tokenize(text)\n","        english_stops = set(stopwords.words('english'))\n","    except:\n","        tokens = text.split()\n","        english_stops = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n","\n","    # Filter tokens\n","    all_stopwords = english_stops.union(set(CUSTOM_STOPWORDS))\n","    filtered_tokens = [\n","        DRUG_CORRECTIONS.get(token, token) for token in tokens\n","        if token not in all_stopwords and token not in string.punctuation\n","    ]\n","\n","    return ' '.join(filtered_tokens)"],"metadata":{"id":"uTmzAGJPyh9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_single_json_file(input_file, output_file, fields_to_process=None):\n","    \"\"\"Process a single JSON file\"\"\"\n","    if fields_to_process is None:\n","        fields_to_process = FIELDS_TO_PROCESS\n","\n","    try:\n","        with open(input_file, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","\n","        # Process each field\n","        for field in fields_to_process:\n","            if field in data and isinstance(data[field], str):\n","                data[f\"{field}_original\"] = data[field]\n","                data[field] = clean_biomedical_text(data[field])\n","\n","        # Save processed data\n","        with open(output_file, 'w', encoding='utf-8') as f:\n","            json.dump(data, f, indent=2, ensure_ascii=False)\n","\n","        return {'status': 'success', 'file': input_file}\n","\n","    except Exception as e:\n","        return {'status': 'error', 'file': input_file, 'error': str(e)}"],"metadata":{"id":"wIlmxvwO0-Zy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_file_wrapper(file_task):\n","    \"\"\"Wrapper for parallel processing\"\"\"\n","    input_file, output_file = file_task\n","    start_time = time.time()\n","\n","    result = process_single_json_file(input_file, output_file)\n","    result['processing_time'] = time.time() - start_time\n","    result['input_file'] = input_file\n","    result['output_file'] = output_file\n","\n","    return result"],"metadata":{"id":"wuh6Xm-f1KIi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collect_all_json_files(input_dir, output_dir):\n","    \"\"\"Collect all JSON files for processing\"\"\"\n","    file_tasks = []\n","\n","    for root, dirs, files in os.walk(input_dir):\n","        relative_path = os.path.relpath(root, input_dir)\n","        output_subdir = output_dir if relative_path == '.' else os.path.join(output_dir, relative_path)\n","        os.makedirs(output_subdir, exist_ok=True)\n","\n","        for f in files:\n","            if f.endswith(\".json\"):\n","                input_file = os.path.join(root, f)\n","                output_file = os.path.join(output_subdir, f)\n","                file_tasks.append((input_file, output_file))\n","\n","    return file_tasks"],"metadata":{"id":"J6l8SfSv1GrQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Preprocessing using Batch Processing**\n","\n","When processing thousands of DailyMed XML files (like the ~100,000+ drug labels in the full database), loading everything into memory at once would crash most systems. Batch processing solves this by:\n","\n","1. Memory efficiency: Process files in small chunks (50-100 files at a time)\n","2. Fault tolerance: One corrupted file won't stop the entire process\n","3. Progress tracking: Monitor completion in real-time\n","4. Scalability: Handle datasets from hundreds to hundreds of thousands of files\n","\n","\n","##### Alternatives:\n","1. ProcessPoolExecutor: CPU-intensive text processing\n","2. ThreadPoolExecutor: I/O-heavy file operations"],"metadata":{"id":"OIpRZS8CVaBR"}},{"cell_type":"code","source":["# Install required packages for parallel processing\n","!pip install tqdm"],"metadata":{"id":"BmfDAJaQFAyY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from concurrent.futures import ProcessPoolExecutor, as_completed\n","from multiprocessing import cpu_count\n","from tqdm import tqdm\n","import threading"],"metadata":{"id":"syY3chJXATxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_json_task_batch(input_dir, output_dir, batch_size=50):\n","    \"\"\"\n","    Process files in batches for optimal parallel processing\n","    \"\"\"\n","    print(\"=\" * 60)\n","    print(\"BATCH PARALLEL PREPROCESSING TASK\")\n","    print(\"=\" * 60)\n","\n","    # Collect all files\n","    file_tasks = collect_all_json_files(input_dir, output_dir)\n","    if not file_tasks:\n","        print(\"No JSON files found to process!\")\n","        return\n","\n","    # Process in batches\n","    total_batches = (len(file_tasks) + batch_size - 1) // batch_size\n","    print(f\"Processing {len(file_tasks)} files in {total_batches} batches of {batch_size}\")\n","\n","    all_successful = []\n","    all_failed = []\n","\n","    for batch_num in range(total_batches):\n","        start_idx = batch_num * batch_size\n","        end_idx = min(start_idx + batch_size, len(file_tasks))\n","        batch_tasks = file_tasks[start_idx:end_idx]\n","\n","        print(f\"\\nProcessing batch {batch_num + 1}/{total_batches} ({len(batch_tasks)} files)\")\n","\n","        # Process current batch\n","        with ProcessPoolExecutor(max_workers=cpu_count()) as executor:\n","            futures = [executor.submit(process_file_wrapper, task) for task in batch_tasks]\n","\n","            for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Batch {batch_num + 1}\"):\n","                result = future.result()\n","\n","                if result['status'] == 'success':\n","                    all_successful.append(result)\n","                else:\n","                    all_failed.append(result)\n","\n","    # Final results\n","    print(f\"\\nTotal successful: {len(all_successful)}\")\n","    print(f\"Total failed: {len(all_failed)}\")\n","\n","    if all_failed:\n","        print(\"Failed files:\")\n","        for failed in all_failed[:5]:  # Show first 5 failures\n","            print(f\"  - {failed['file']}: {failed.get('error', 'Unknown error')}\")"],"metadata":{"id":"vnIIQFQFExqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["json_input_dir = output_folder / 'xml_files'\n","output_dir = output_folder / 'json_clean_files'\n","\n","# Batch processing for very large datasets\n","preprocess_json_task_batch(json_input_dir, output_dir, batch_size=100)"],"metadata":{"id":"goPQJMPXKiDx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"2\"></a>\n","\n","## 2. Named Entity Recognition (NER) + Named Entity Linking (NEL)\n","\n","**Goal**: To recognize drugs, chemicals, and disease entities in the retrieved datasets and to link them to the respective ontology identifiers.\n","\n","We are going to use the [Chemical Entities of Biological Interest](https://www.ebi.ac.uk/chebi/) (ChEBI), [Disease Ontology](https://disease-ontology.org/) (DO), and the [Orphanet](https://www.orpha.net/).\n","To perform NER and NEL, we are going to apply Minimal Named-Entity Recognizer [MER](https://pypi.org/project/merpy/) tool."],"metadata":{"id":"cjDy-gEUw-ae"}},{"cell_type":"markdown","source":["<a id=\"2.1\"></a>\n","### 2.1. Import libraries"],"metadata":{"id":"QCcWMw7WyTkS"}},{"cell_type":"code","source":["!pip3 install ssmpy"],"metadata":{"id":"skEDVu1Bw7Yw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip3 install merpy"],"metadata":{"id":"z5-wcEtG6iAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import merpy\n","import multiprocessing\n","from collections import Counter"],"metadata":{"id":"RNSsxsVJyZUG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"2.2\"></a>\n","### 2.2. Configure MER\n","\n","\n","First, we are going to download the **ChEBI** and **Disease Ontology (DO)** ontologies:\n","\n","**IMPORTANT:**  Only for OSX\n","see: https://github.com/prisma-labs/python-graphql-client/issues/13"],"metadata":{"id":"wIaT3VUwzXEL"}},{"cell_type":"code","source":["import urllib.request\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","response = urllib.request.urlopen('https://www.python.org')"],"metadata":{"id":"ugBzQbV0zmDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the base directory for our project\n","BASE_DIR = Path(os.getcwd())\n","\n","# Set up output folder paths\n","output_folder = BASE_DIR / 'json'"],"metadata":{"id":"oQHhS0tW3hzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ontologies_dir = BASE_DIR / \"ontologies\"\n","os.makedirs(ontologies_dir, exist_ok=True)\n","os.chdir(ontologies_dir)"],"metadata":{"id":"bYlcHR_Q0PCG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lexicon_name = 'doid'\n","try:\n","    # Try to load existing lexicon\n","    merpy.get_entities(\"test\", lexicon_name)\n","    print(f\"{lexicon_name} lexicon already exists\")\n","except:\n","    # Download if doesn't exist or is corrupted\n","    # Download DO (2025-06-27 version)\n","    print(f\"Downloading {lexicon_name} lexicon...\")\n","    merpy.download_lexicon('http://purl.obolibrary.org/obo/doid.owl',\n","                          'doid', ltype='owl')\n","    print(f\"{lexicon_name} lexicon downloaded\")"],"metadata":{"id":"i97CtMeZ0yt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lexicon_name = 'chebi'\n","try:\n","    # Try to load existing lexicon\n","    merpy.get_entities(\"test\", lexicon_name)\n","    print(f\"{lexicon_name} lexicon already exists\")\n","except:\n","    # Download if doesn't exist or is corrupted\n","    # Download ChEBI (2021-07-01 version)\n","    print(f\"Downloading {lexicon_name} lexicon...\")\n","    merpy.download_lexicon('http://purl.obolibrary.org/obo/chebi/chebi_lite.owl',\n","                       'chebi', ltype='owl')\n","    print(f\"{lexicon_name} lexicon downloaded\")"],"metadata":{"id":"wJlI-8OxFMFj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install gawk using apt-get (system package manager)\n","!apt-get update\n","!apt-get install -y gawk"],"metadata":{"id":"up9vtR6p8Iv3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Then, we need to process the downloaded files into lexicons that MER can use\n","merpy.process_lexicon(\"doid\", ltype=\"owl\")\n","merpy.process_lexicon(\"chebi\", ltype=\"owl\")"],"metadata":{"id":"IG9x2CX66MCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We are going to delete obsolete concepts still present in the ontologies file:\n","merpy.delete_obsolete(\"chebi\")\n","merpy.delete_obsolete(\"doid\")"],"metadata":{"id":"Plk3w6pu8OlX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's check the lexicons available for MER:\n","merpy.show_lexicons()"],"metadata":{"id":"7Rz4l0T58W2d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"2.3\"></a>\n","### 2.3. Extract the entities in a sample"],"metadata":{"id":"5h4MURrianW9"}},{"cell_type":"code","source":["# Example 1\n","json_file_path = Path(output_folder) / \"xml_files/0d1f10eb-de2e-49c3-bbea-2cafbafe05b1.json\"\n","\n","#  Load JSON data\n","with open(json_file_path) as f:\n","    drugs_data = json.load(f)"],"metadata":{"id":"KE8uiXr8ahO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's check the contents of the file:\n","print(drugs_data.keys())"],"metadata":{"id":"gbNkK3N5Z2Pi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We want to recognize the entities present in indications and ingredients.\n","indications_test = drugs_data[\"indications\"]\n","merpy.get_entities(indications_test, 'doid')"],"metadata":{"id":"bw-br7wHbhMj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's check if the annotations make sense. For instance, access the link http://purl.obolibrary.org/obo/DOID_1184."],"metadata":{"id":"t9VkrrnP4fFc"}},{"cell_type":"markdown","source":["The entity 'nephrotic syndrome' in the file was linked to the DO concept 'nephrotic syndrome' with the identifier 'DOID:1184', which seems correct!"],"metadata":{"id":"vif5UpYq4p_q"}},{"cell_type":"markdown","source":["Let's apply MER to recognize chemical entities and to link them to ChEBI concepts:"],"metadata":{"id":"gWAxa-4q5COR"}},{"cell_type":"code","source":["merpy.get_entities(indications_test, 'chebi')"],"metadata":{"id":"IlJkjPQQcHUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Accessing the link http://purl.obolibrary.org/obo/CHEBI_4027, we can see that the entity 'Cyclophosphamide' was linked to the ChEBI concept 'steroid', which has the identifier 'CHEBI:4027'."],"metadata":{"id":"tqEJZ2fW5J6W"}},{"cell_type":"code","source":["ingredients_test = drugs_data[\"ingredients\"]\n","merpy.get_entities(ingredients_test, 'chebi')"],"metadata":{"id":"CHFFYvSFcEEy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ingredients_test = drugs_data[\"ingredients\"]\n","print(ingredients_test)"],"metadata":{"id":"D5M7jG4XcbLq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merpy.get_entities(ingredients_test[0][\"name\"], 'chebi')"],"metadata":{"id":"x9DxjQ6Cc5Pk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"2.4\"></a>\n","### 2.4. Others ontologies\n","\n","First, we are going to download the [**Orphanet (ORDO)**](https://www.orpha.net/) ontology"],"metadata":{"id":"GiDKsr2qeT36"}},{"cell_type":"code","source":["!pip3 install owlready2"],"metadata":{"id":"IiASnx8LR0OJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk import pos_tag\n","from owlready2 import get_ontology\n","from difflib import SequenceMatcher"],"metadata":{"id":"ELXGqSEZRv7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir(ontologies_dir)"],"metadata":{"id":"rmZC8mVw3927"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Version 4.7 (english) - 30 Jun 2025\n","try:\n","    # Try to load existing lexicon\n","    onto = get_ontology(\"https://www.orphadata.com/data/ontologies/ordo/last_version/ORDO_en_4.7.owl\").load()\n","    print(\"Ontology loaded successfully.\")\n","except Exception as e:\n","    print(f\"Error loading ontology: {e}\")"],"metadata":{"id":"mSJ7FwvbSbhB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build a comprehensive mapping of disease terms to ontology IRIs\n","disease_terms = {}\n","synonym_count = 0\n","label_count = 0\n","\n","# Iterate through all classes in the ontology\n","for cls in onto.classes():\n","    # Add primary labels\n","    if hasattr(cls, \"label\") and cls.label:\n","        for label in cls.label:\n","            disease_terms[label.lower()] = cls.iri\n","            label_count += 1\n","\n","    # Add synonyms of different types\n","    for prop in [\"hasExactSynonym\", \"hasRelatedSynonym\", \"hasNarrowSynonym\"]:\n","        if hasattr(cls, prop):\n","            for synonym in getattr(cls, prop):\n","                disease_terms[synonym.lower()] = cls.iri\n","                synonym_count += 1"],"metadata":{"id":"zFgB5UB5VFeq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example 2\n","sample_text = \"\"\"\n","    The patient was diagnosed with Niemann-Pick disease type C.\n","    There were no signs of cystic fibrosis. However, we cannot rule out\n","    a mild form of Gaucher disease based on the enzyme analysis.\n","    \"\"\""],"metadata":{"id":"mjd_ovCbwR9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract potential disease mentions from text using regular expression patterns.\n","\n","# Define patterns for disease recognition\n","disease_patterns = [\n","    # Common disease suffixes\n","    r'\\b\\w+(?:disease|syndrome|disorder|deficiency|infection|malignancy|cancer|leukemia|lymphoma)\\b',\n","\n","    # Diseases with modifiers\n","    r'(?:acute|chronic|severe|invasive)\\s+\\w+\\s+\\w+',\n","\n","    # Hyphenated disease names\n","    r'\\b\\w+-\\w+\\s+(?:disease|syndrome|disorder|infection)\\b',\n","\n","    # Specific complex disease patterns\n","    r'\\b\\w+\\s+versus-host\\s+disease\\b',\n","\n","    # Explicitly named diseases\n","    r'\\bNiemann-Pick\\s+disease\\b',\n","    r'\\bGaucher\\s+disease\\b',\n","    r'\\bFabry\\s+disease\\b',\n","    r'\\bCystic\\s+fibrosis\\b',\n","    r'\\bHuntington\\'s\\s+disease\\b',\n","    r'\\bAlzheimer\\'s\\s+disease\\b',\n","    r'\\bParkinson\\'s\\s+disease\\b'\n","]\n","\n","# Find all matches for each pattern\n","matches = []\n","for pattern in disease_patterns:\n","    matches.extend(re.findall(pattern, sample_text, re.IGNORECASE))\n","\n","# Remove duplicates and return\n","unique_matches = list(set(matches))\n","print(f\"Extracted {len(unique_matches)} potential disease entities\")\n","print(unique_matches)"],"metadata":{"id":"nGWA4B3PWrt4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["similarity_threshold = 0.8\n","\n","# Process each disease with inline matching\n","for disease in unique_matches:\n","    # Inline ontology matching - replaces find_disease_in_ontology function\n","    search_name = disease.lower().strip()\n","    ontology_id = None\n","\n","    # Try exact match first\n","    if search_name in disease_terms:\n","        ontology_id = disease_terms[search_name]\n","        print(f\"Exact match: {disease} -> {ontology_id}\")\n","    else:\n","        # Fuzzy matching with similarity scoring\n","        best_match_iri = None\n","        highest_score = 0.0\n","        best_term = None\n","\n","        for term, iri in disease_terms.items():\n","            score = SequenceMatcher(None, search_name, term).ratio()\n","            if score > highest_score and score > similarity_threshold:\n","                best_match_iri = iri\n","                highest_score = score\n","                best_term = term\n","\n","        if best_match_iri:\n","            ontology_id = best_match_iri\n","            print(f\"Fuzzy match: {disease} -> {best_term} -> {ontology_id} (score: {highest_score:.2f})\")\n","        else:\n","            print(f\"No match found for: {disease}\")\n","\n","    # Use ontology_id here for further processing\n","    if ontology_id:\n","        # Your processing logic here\n","        print(f\"Processing {disease} with ID: {ontology_id}\")\n","    else:\n","        print(f\"Skipping {disease} - no ontology mapping found\")"],"metadata":{"id":"tQReaU6VXbnM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we are going to iterate over each file present in the sample directory, annotate it, and create the respective entity file"],"metadata":{"id":"9LYDvf9anjeu"}},{"cell_type":"markdown","source":["Now we have both the files ('data/json' directory) and the respective entities files ('data/ner' directory), and the next step will be the creation of the **Knowledge Graph**."],"metadata":{"id":"Wz2jjkFz93hU"}},{"cell_type":"markdown","source":["<a id=\"3\"></a>\n","## 3. Knowledge Graph\n","\n","**Goal**: Build a pharmaceutical Knowledge Graph linking drugs, diseases, administration routes, and approval years. Using normalized JSON data, we create nodes (Drug, Disease, AdminRoute, ApprovalYear) and relationships such as `HAS_INDICATION`, `HAS_CONTRAINDICATION`, `HAS_ADMIN_ROUTE`, and `HAS_APPROVAL_YEAR`.\n","\n","Identifiers come from [DrugBank](https://go.drugbank.com/), [Chemical Entities of Biological Interest](https://www.ebi.ac.uk/chebi/) (ChEBI), [Disease Ontology](https://disease-ontology.org/) (DO), and the [Orphanet](https://www.orpha.net/).\n","\n","This step uses the official Python Neo4j driver to insert the entities and enforce uniqueness constraints, enabling efficient queries across the graph.\n","\n","\n","‚ö†Ô∏è **IMPORTANT**:\n","\n","You need to create a Neo4j account first. Follow these steps:\n","1. Sign up for Neo4j with your credentials.\n","2. Create a free AuraDB instance.\n","3. After selecting that option, you will see the credentials for `Instance01`. Click `Download and Continue`.\n","4. Open the downloaded `.txt` file and copy the values for `NEO4J_URI`, `NEO4J_USERNAME`, and `NEO4J_PASSWORD` to use in step 3.2."],"metadata":{"id":"w1stnZeunlxI"}},{"cell_type":"markdown","source":["\n","<a id=\"3.1\"></a>\n","### 3.1. Import Libraries\n"],"metadata":{"id":"4CpRV2jPBCkk"}},{"cell_type":"code","source":["!pip install neo4j"],"metadata":{"id":"-Eqhw58-N8GA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","from neo4j import GraphDatabase\n","from datetime import datetime"],"metadata":{"id":"inc4Dp3W-o8-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"3.2\"></a>\n","### 3.2. Enter your Neo4j credentials"],"metadata":{"id":"-omOc_zfCDFy"}},{"cell_type":"code","source":["# Section 2: Use your Neo4j credentials\n","uri = \"bolt://localhost:7687\"      # Neo4j server address\n","user = \"neo4j\"                     # Neo4j username\n","password = \"your_password\"         # Neo4j passowrd"],"metadata":{"id":"ai2Brj-pBbC-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"3.3\"></a>\n","### 3.3. Neo4j Handler class"],"metadata":{"id":"W_DupoPtETJQ"}},{"cell_type":"code","source":["class Neo4jHandler:\n","    def __init__(self, uri, user, password):\n","        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n","    def close(self):\n","        self.driver.close()\n","    def create_constraints(self):\n","        with self.driver.session() as session:\n","            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (d:Drug) REQUIRE d.id IS UNIQUE\")\n","            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (disease:Disease) REQUIRE disease.id IS UNIQUE\")\n","            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (r:AdminRoute) REQUIRE r.name IS UNIQUE\")\n","            session.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (y:ApprovalYear) REQUIRE y.year IS UNIQUE\")\n","    def create_drug(self, drug):\n","        query = \"\"\"\n","        MERGE (d:Drug {id: $id})\n","        SET d.name = $name, d.organization = $organization, d.effectiveTime = $effectiveTime\n","        \"\"\"\n","        with self.driver.session() as session:\n","            session.run(query, **drug)\n","    def create_disease(self, disease):\n","        query = \"\"\"\n","        MERGE (d:Disease {id: $id})\n","        SET d.name = $name\n","        \"\"\"\n","        with self.driver.session() as session:\n","            session.run(query, **disease)\n","    def create_admin_route(self, route, drug_id):\n","        if not route:\n","            return\n","        routes = [r.strip() for r in route.split(\",\")]\n","        with self.driver.session() as session:\n","            for r in routes:\n","                query = \"\"\"\n","                MERGE (r:AdminRoute {name: $route})\n","                WITH r\n","                MATCH (d:Drug {id: $drug_id})\n","                MERGE (d)-[:ADMINISTERED_VIA]->(r)\n","                \"\"\"\n","                session.run(query, route=r, drug_id=drug_id)\n","    def create_approval_year(self, year, drug_id):\n","        if not year:\n","            return\n","        query = \"\"\"\n","        MERGE (y:ApprovalYear {year: $year})\n","        WITH y\n","        MATCH (d:Drug {id: $drug_id})\n","        MERGE (d)-[:APPROVED_IN]->(y)\n","        \"\"\"\n","        with self.driver.session() as session:\n","            session.run(query, year=year, drug_id=drug_id)\n","    def create_relationships(self, drug_id, indications, contraindications, route, approval_year):\n","        with self.driver.session() as session:\n","            for ind in indications:\n","                session.run(\"\"\"\n","                MATCH (d:Drug {id: $drug_id}), (dis:Disease {id: $dis_id})\n","                MERGE (d)-[:TREATS]->(dis)\n","                \"\"\", {\"drug_id\": drug_id, \"dis_id\": ind[\"id\"]})\n","            for con in contraindications:\n","                session.run(\"\"\"\n","                MATCH (d:Drug {id: $drug_id}), (dis:Disease {id: $dis_id})\n","                MERGE (d)-[:CONTRAINDICATED_FOR]->(dis)\n","                \"\"\", {\"drug_id\": drug_id, \"dis_id\": con[\"id\"]})\n","            if route:\n","                self.create_admin_route(route, drug_id)\n","            if approval_year:\n","                self.create_approval_year(approval_year, drug_id)\n","    def insert_data(self, drug, diseases, indications, contraindications):\n","        self.create_drug(drug)\n","        for disease in diseases:\n","            self.create_disease(disease)\n","        self.create_admin_route(drug.get(\"admin_route\"), drug[\"id\"])\n","        self.create_approval_year(drug.get(\"approval_year\"), drug[\"id\"])\n","        self.create_relationships(drug[\"id\"], indications, contraindications, drug.get(\"admin_route\"), drug.get(\"approval_year\"))"],"metadata":{"id":"GWhQ-XtWEYgX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"3.4\"></a>\n","### 3.4. Utility functions for data processing"],"metadata":{"id":"o4lYyQHTJXaD"}},{"cell_type":"code","source":["def load_json(file_path):\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        return json.load(f)\n","\n","def clean_id(url):\n","    if not url or not isinstance(url, str):\n","        return None\n","    if url.startswith('http://purl.obolibrary.org/obo/'):\n","        return url.split('/')[-1]\n","    return url.split('/')[-1]\n","\n","def extract_year(approval_date):\n","    if not approval_date:\n","        return None\n","    if isinstance(approval_date, int):\n","        return str(approval_date)[:4]\n","    if isinstance(approval_date, str):\n","        try:\n","            parsed_date = datetime.strptime(approval_date, \"%b %d, %Y\")\n","            return str(parsed_date.year)\n","        except ValueError:\n","            try:\n","                parsed_date = datetime.strptime(approval_date, \"%B %d, %Y\")\n","                return str(parsed_date.year)\n","            except ValueError:\n","                pass\n","        if approval_date.isdigit():\n","            if len(approval_date) == 8:\n","                return approval_date[:4]\n","            elif len(approval_date) == 4:\n","                return approval_date\n","    return None\n","\n","def process_xml_file(file_path, neo4j_handler):\n","    try:\n","        data = load_json(file_path)\n","        if \"ingredients\" in data:\n","            if not data.get(\"ingredients\"):\n","                return\n","            main_ingredient = data[\"ingredients\"][0]\n","            chebi_id = clean_id(main_ingredient.get(\"chebi_id\"))\n","            drugbank_id = clean_id(main_ingredient.get(\"drugbank_id\"))\n","            drug_id = chebi_id or drugbank_id\n","            if not drug_id:\n","                return\n","            drug = {\n","                \"id\": drug_id,\n","                \"name\": main_ingredient.get(\"name\"),\n","                \"organization\": data.get(\"organization\"),\n","                \"effectiveTime\": data.get(\"effectiveTime\"),\n","                \"admin_route\": data.get(\"admin_route\"),\n","                \"approval_year\": extract_year(data.get(\"approval_date\")),\n","                \"chebi_id\": chebi_id,\n","                \"drugbank_id\": drugbank_id\n","            }\n","            diseases = []\n","            indications = []\n","            contraindications = []\n","            if \"indications\" in data and isinstance(data[\"indications\"], dict):\n","                for entity in data[\"indications\"].get(\"doid_entities\", []):\n","                    if isinstance(entity, dict):\n","                        disease_id = clean_id(entity.get(\"doid_id\"))\n","                        if disease_id:\n","                            disease_name = entity.get(\"text\", \"Unknown\")\n","                            if \" (DOID:\" in disease_name:\n","                                disease_name = disease_name.split(\" (DOID:\")[0]\n","                            diseases.append({\"id\": disease_id, \"name\": disease_name})\n","                            indications.append({\"id\": disease_id})\n","                for entity in data[\"indications\"].get(\"orphanet_entities\", []):\n","                    if isinstance(entity, dict):\n","                        disease_id = clean_id(entity.get(\"id\"))\n","                        if disease_id:\n","                            disease_name = entity.get(\"text\", \"Unknown\")\n","                            diseases.append({\"id\": disease_id, \"name\": disease_name})\n","                            indications.append({\"id\": disease_id})\n","            if \"contraindications\" in data and isinstance(data[\"contraindications\"], dict):\n","                for entity in data[\"contraindications\"].get(\"doid_entities\", []):\n","                    if isinstance(entity, dict):\n","                        disease_id = clean_id(entity.get(\"doid_id\"))\n","                        if disease_id:\n","                            disease_name = entity.get(\"text\", \"Unknown\")\n","                            if \" (DOID:\" in disease_name:\n","                                disease_name = disease_name.split(\" (DOID:\")[0]\n","                            diseases.append({\"id\": disease_id, \"name\": disease_name})\n","                            contraindications.append({\"id\": disease_id})\n","                for entity in data[\"contraindications\"].get(\"orphanet_entities\", []):\n","                    if isinstance(entity, dict):\n","                        disease_id = clean_id(entity.get(\"id\"))\n","                        if disease_id:\n","                            disease_name = entity.get(\"text\", \"Unknown\")\n","                            diseases.append({\"id\": disease_id, \"name\": disease_name})\n","                            contraindications.append({\"id\": disease_id})\n","            print(f\"Drug: {drug_id} (CHEBI: {chebi_id}, DrugBank: {drugbank_id})\")\n","            print(f\"Admin Route: {drug.get('admin_route')}, Approval Year: {drug.get('approval_year')}\")\n","            print(f\"Found {len(diseases)} diseases, {len(indications)} indications, {len(contraindications)} contraindications\")\n","            neo4j_handler.insert_data(drug, diseases, indications, contraindications)\n","        else:\n","            for drug_entry in data.get(\"drug\", []):\n","                chebi_id = clean_id(drug_entry.get(\"chebi_id\"))\n","                drugbank_id = clean_id(drug_entry.get(\"drugbank_id\"))\n","                drug_id = chebi_id or drugbank_id\n","                if not drug_id:\n","                    continue\n","                approval_date = (\n","                    data.get(\"Approval_Date\") or\n","                    data.get(\"Approval Date\") or\n","                    drug_entry.get(\"approval_date\") or\n","                    data.get(\"effectiveTime\") or\n","                    data.get(\"date\")\n","                )\n","                approval_year = extract_year(approval_date)\n","                admin_route = drug_entry.get(\"admin_route\") or data.get(\"Route of Administration\")\n","                drug = {\n","                    \"id\": drug_id,\n","                    \"name\": drug_entry.get(\"name\"),\n","                    \"organization\": data.get(\"organization\") or data.get(\"Applicant\"),\n","                    \"effectiveTime\": data.get(\"effectiveTime\"),\n","                    \"admin_route\": admin_route,\n","                    \"approval_year\": approval_year,\n","                    \"chebi_id\": chebi_id,\n","                    \"drugbank_id\": drugbank_id\n","                }\n","                diseases = []\n","                indications = []\n","                contraindications = []\n","                for ind in data.get(\"indications\", []):\n","                    doid_id = clean_id(ind.get(\"doid_id\"))\n","                    orphanet_id = clean_id(ind.get(\"orphanet_id\"))\n","                    disease_id = doid_id or orphanet_id\n","                    if disease_id:\n","                        diseases.append({\"id\": disease_id, \"name\": ind.get(\"text\", \"Unknown\")})\n","                        indications.append({\"id\": disease_id})\n","                for con in data.get(\"contraindications\", []):\n","                    doid_id = clean_id(con.get(\"doid_id\"))\n","                    orphanet_id = clean_id(con.get(\"orphanet_id\"))\n","                    disease_id = doid_id or orphanet_id\n","                    if disease_id:\n","                        diseases.append({\"id\": disease_id, \"name\": con.get(\"text\", \"Unknown\")})\n","                        contraindications.append({\"id\": disease_id})\n","                print(f\"Drug: {drug_id} (CHEBI: {chebi_id}, DrugBank: {drugbank_id})\")\n","                print(f\"Admin Route: {admin_route}, Approval Year: {approval_year}\")\n","                print(f\"Found {len(diseases)} diseases, {len(indications)} indications, {len(contraindications)} contraindications\")\n","                neo4j_handler.insert_data(drug, diseases, indications, contraindications)\n","    except Exception as e:\n","        print(f\"Error processing file '{file_path}': {e}\")"],"metadata":{"id":"eE32P2jdJXDI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"3.5\"></a>\n","### 3.5. Establish connection to Neo4j\n"],"metadata":{"id":"zFqF8ZqXEjxd"}},{"cell_type":"code","source":["# Section 4: Create a Neo4j handler and define constraints\n","neo4j_handler = Neo4jHandler(uri, user, password)\n","neo4j_handler.create_constraints()\n","print(\"Established connection and created constraints.\")"],"metadata":{"id":"7nzKLZvFErlq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"3.6\"></a>\n","### 3.6. Import data from JSON files\n"],"metadata":{"id":"aEXq47K-E2Ua"}},{"cell_type":"code","source":["# Path to the project's JSON folder\n","entities_path = BASE_DIR / 'entities'\n","\n","# List all contents of the folder recursively\n","print(f\"Listing all contents of {entities_path} (recursively):\")\n","for path in entities_path.rglob('*'):\n","    print(path)\n","\n","# Recursively search for JSON files within the project\n","json_files = list(entities_path.rglob('*.json'))\n","\n","if not json_files:\n","    print(f\"No JSON files found in {entities_path}\")\n","else:\n","    print(f\"Found {len(json_files)} JSON files:\")\n","    for json_file in json_files:\n","        print(f\"  - {json_file}\")\n","\n","    # Import function (make sure process_xml_file and neo4j_handler are defined)\n","    def import_data(file_path):\n","        process_xml_file(file_path, neo4j_handler)\n","        print(f\"Import completed for file: {file_path}\")\n","\n","    # Process all found JSON files\n","    for json_file in json_files:\n","        import_data(str(json_file))\n"],"metadata":{"id":"gsn--CgDFDYF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a id=\"3.7\"></a>\n","### 3.7. Close Neo4j connection\n"],"metadata":{"id":"tAqiAMzkIDoa"}},{"cell_type":"code","source":["# Close Neo4j connection\n","neo4j_handler.close()\n","print(\"Neo4j connection closed\")"],"metadata":{"id":"5vQwwlaXJ2AC"},"execution_count":null,"outputs":[]}]}